<!DOCTYPE html>
<html>
<head>
    <title>Daily arXiv Subscriptions</title>
    <style>
        body { font-family: Arial, sans-serif; }
        h1 { color: #333; }
        .paper { margin-bottom: 20px; }
    </style>
</head>
<body>
    <h1>Daily arXiv Subscriptions</h1>
    
    <div class="paper">
        <h2><a href="">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a></h2>
        <p><strong>Category:</strong> </p>
        <p><strong>Authors:</strong> Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han</p>
        <p><strong>Abstract:</strong> LServe is an efficient system that accelerates long-context LLM serving via hybrid sparse attention and dynamic hierarchical KV page pruning, achieving up to 2.9x prefilling and 1.3-2.1x decoding speedups over vLLM while maintaining accuracy, with code available at https://github.com/mit-han-lab/omniserve.</p>
    </div>
    
    <div class="paper">
        <h2><a href="">Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework</a></h2>
        <p><strong>Category:</strong> </p>
        <p><strong>Authors:</strong> Yuming Yang, Jiang Zhong, Li Jin, Jingwang Huang, Jingpeng Gao, Qing Liu, Yang Bai, Jingyuan Zhang, Rui Jiang, Kaiwen Wei</p>
        <p><strong>Abstract:</strong> This paper introduces Chart-based MRAG and the CHARGE framework to generate evaluation data, constructing the Chart-MRAG Bench benchmark with 4,738 QA pairs, which reveals current methods' limitations in retrieval performance, low accuracy (58.19% Correctness) even with ground-truth data, and consistent text-over-visual bias in multimodal large language models.</p>
    </div>
    
    <div class="paper">
        <h2><a href="">Interpretable Text Embeddings and Text Similarity Explanation: A Primer</a></h2>
        <p><strong>Category:</strong> </p>
        <p><strong>Authors:</strong> Juri Opitz, Lucas MÃ¶ller, Andrianos Michail, Simon Clematide</p>
        <p><strong>Abstract:</strong> This paper provides a structured overview and evaluation of emerging interpretability methods that explain similarity scores in text embeddings, addressing transparency challenges crucial for AI and NLP systems, particularly in search applications.</p>
    </div>
    
</body>
</html>